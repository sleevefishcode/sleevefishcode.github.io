---
title: 深度学习常见概念
date: 2022-04-04 20:29:47
categories: python
tags: 深度学习
---

最近学习了CBAM相关的内容，对注意力机制中常见概念记录一下。
<!--more-->

#### 介绍

卷积神经网络凭借其强大的特征提取和表达能力，在计算机视觉任务中取得了很好的应用效果，为了进一步提升CNNs的性能，近来的方法会从三个方面考虑：**深度，宽度，基数。**

在深度方面的探索由来已久，VGGNet证明，堆积相同形状的卷积块能取得不错的效果，基于同样的思想，ResNet在堆积的基础上，加入skip connection，通过残差学习，在保证性能不退化的基础上，大大加深了网络层数。![v2-4f61de5ca32841b3a94b692cafab2ff9_1440w](深度学习常见概念/v2-4f61de5ca32841b3a94b692cafab2ff9_1440w.jpg)

在网络宽度方面，GoogleNet是一个重要尝试，该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），增加了网络的宽度。![v2-e50b382e6b27206a1d6820def4d09b7a_1440w](深度学习常见概念/v2-e50b382e6b27206a1d6820def4d09b7a_1440w.jpg)

在网络基数方面，Facebook家的ResNeXt是一个很好的工作，何为基数（cardinality）呢？Cardinatity指的是一个block中所具有的相同分支的数目。ResNeXt通过一系列实验证明了，加大基数能够达到甚至超越加大宽度和深度的效果，具有更强的特征表现能力。![v2-7d250d62d95d7eaa6117df7c6f59b6f2_1440w](深度学习常见概念/v2-7d250d62d95d7eaa6117df7c6f59b6f2_1440w.jpg)

#### **空间域（ Spatial Domain）**

空间域的设计思路是通过注意力机制，更关注的是位置特性。将原始图片中的空间信息通过空间转换模块，变换到另一个空间中并保留关键信息。

#### **通道域（Channel Domain）**

通道域的核心思想是，对经过卷积得到的特征图的每一层，乘以不同的权重，表示该层表示的特征对于关键信息的关联程度和重要程度，相应的，权重越大，表示该层表示的信息对于关键信息越重要，关联程度越高；权重越小，表示该层表示的信息对于关键信息越不重要。SeNet是典型的注意力机制模型，得到一个C维卷积层后，通过挤压函数，激励函数，尺度函数，得到每一维的权重，对应乘到不同通道的值上，得到新的特征。

**注意力机制不仅告诉你应该关注哪里，而且还会提升关键区域的特征表达**。这也与识别的目标一致，只关注重要的特征而抑制或忽视无关特征。这样的思想，促成了本文提出的CBAM网络（Convolutional Block Attention Module）。如下图所示，依次用到注意力机制中的通道域模块和空间域模块，通过这两个模块，得到细化后的feature。网络具有了学习“What”和“Where”的能力，让网络更好的知道哪些信息需要强调，哪些信息需要抑制。

- **Channel attention module.**

**核心思想：**利用特征的通道间关系，生成通道注意图

**输入**：C维 feature map

**输出**：1x1xC channel attention map

**步骤：**

**（1）**对于输入feature map，分别进行平均池化和最大值池化聚合空间信息，得到两个C维池化特征图，F_avg和F_max。

**（2）**将F_avg和F_max送入包含一个隐层的多层感知器MLP里，得到两个1x1xC的通道注意力图。其中，为了减少参数量，隐层神经元的个数为C/r，r也被称作压缩比。

**（3）**将经过MLP得到的两个通道注意力图进行对应元素相加，激活，得到最终的通道注意力图Mc。

*MLP理解：https://blog.csdn.net/sdsc1314/article/details/106742705*

**几个答疑**：

**（1）为什么要对输入特征进行平均池化和最大池化两种处理方式？**

为了更好的聚合feature map的信息并减少参数量，池化是必不可少的步骤，对于空间信息的聚合，目前普遍采用的是平均池化的方法，但作者认为，最大池化会收集到不同于平均池化的，关于不同目标特征的表示，这对于后续得到更精细的注意力通道图是有帮助的。实验也表明，对于平均池化和最大池化的综合运用，是有助于性能提升的。

**（2）MLP的结构是如何设计的？**

本文用到的是只有一层hidden layer的MLP，非常简单，以W0和W1分别表示隐层权重和输出层权重，那么属性可以表示为：

![img](深度学习常见概念/v2-89f01a059e33f95f7df961868f829c98_720w.png)

此外，W0和W1的参数是共享的。

**综上，通道注意力模块更关注的是“What”属性，也就是什么是对于后续处理有意义的。经过Channel Attention Module之后，我们得到的是一个1x1xC的通道注意力图，图上每一维的权重，表示该维对应的feature map层中，对于关键信息的重要程度和关联程度。**

- **Spatial attention module.**

**核心思想：**利用特征间的空间关系生成空间注意图

**输入**：Channel-refined feature **F’**（经过通道注意力图细化后的feature map）

**输出**：HxW的spatial map

**步骤：**

**（1）**对于F’首先沿着通道方向进行最大池化和平均池化，得到两个二维的feature map F_avg和F_max，属性都是1xHxW，将得到的两个feature map进行维度拼接（concatenate），得到拼接后的feature map。

**（2）**对于拼接后的feature map，利用size为7x7的卷积层生成空间注意力图Ms。

**公式表示：**

![img](https://pic3.zhimg.com/80/v2-c5ba8e3165eef444b1393b467e7442b6_720w.jpg)

![img](深度学习常见概念/v2-aeb1ee25a324e80863d977924acb5db5_720w.jpg)



**几个答疑：**

**（1）为什么要沿着维度通道进行平均池化和最大池化？**

作者借鉴ICLR2017的论文《Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer》，认为沿着通道轴应用池化操作可以有效地突出显示含有关键信息的区域。作者的实验也支持这一观点。

**（2）维度拼接（Concatenate）具体操作过程？**

Concatenate的具体操作是将两个size相同的feature map，在维度方向进行拼接，拼接完成后，新的特征图的通道会是原来两个特征图通道之和。这种操作是进行特征融合的普遍做法。

**综上，空间注意力模块更关注的是“Where”，即哪些位置有关键信息，这对于前面的关注“What”的通道注意力模块是一个补充，成为网络的左膀右臂。**

- **Arrangement of attention modules**

我们介绍了通道注意力模块和空间注意力模块，两个模块用什么方式组合是最优的呢？并行还是连续，不同的组合方式会影响网络的表现，作者做了一系列实验表明，**顺序排列比并行排列的结果更好。**

#### Faster R-CNN-FPN

解读：http://events.jianshu.io/p/aa659406d8c4

#### 卷积核和过滤器的区别

https://medlen.blog.csdn.net/article/details/109906338

[深度可分离卷积 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/92134485)

- [卷积核](https://so.csdn.net/so/search?q=卷积核&spm=1001.2101.3001.7020)就是由长和宽来指定的，是一个二维的概念。
- 而过滤器是是由长、宽和深度指定的，是一个三维的概念。
- 过滤器可以看做是卷积核的[集合](https://so.csdn.net/so/search?q=集合&spm=1001.2101.3001.7020)。
- 过滤器比卷积核高一个[维度](https://so.csdn.net/so/search?q=维度&spm=1001.2101.3001.7020)——深度。
- 卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3
- ![img](深度学习常见概念/aow_drv.log)

#### 1x1卷积核作用

https://zhuanlan.zhihu.com/p/40050371

**降维/升维**

#### 池化

https://zhuanlan.zhihu.com/p/58381421

#### 前向传播和反向传博算法

https://blog.csdn.net/bitcarmanlee/article/details/78819025

#### resnet-mmdet解读

https://blog.csdn.net/cc__cc__/article/details/105470864?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2.pc_relevant_default&utm_relevant_index=5

一、ResNet网络中的两种基本残差块
由网络结构图可以看出，ResNet-18和ResNet-34使用的为包含2个3×3卷积层的残差块，在源码中对应BasicBlock
而更深层的ResNet-50，ResNet-101，ResNet-152使用的残差块则略微复杂一些，在源码中对应Bottleneck

![在这里插入图片描述](深度学习常见概念/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjX19jY19f,size_16,color_FFFFFF,t_70#pic_center)